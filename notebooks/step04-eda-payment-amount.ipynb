{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Payment Amount\n",
    "- This EDA  is being done by Lori & Teresa.\n",
    "\n",
    "Covered in this notebook:\n",
    "1. First approach: Teresa and I were concerned that outliers have been 'buried' in the combined/grouped dfs. We went back to working with 2015 data (me) and 2016 data (Teresa) to see if we can find outliers that way.\n",
    "2. That approach didn't yeild results we could use, so we then started to look at the best way to get to the business question were were attempting to answer: Find the change between 2015 and 2017 in average allowed amount, based on three criteria (payment_type = Doctor Only, Facility Only, or Doctor & Facility; NPI and HCPCS code)\n",
    "- We attempted to do that in Python, but ran into problems with the data (the '201x' column name was an integer, not a string!).  After resolving that, though, we still couldn't find the difference between 2015 adn 2017.\n",
    "- After consulting with the rest of the team (who had run into similar problems), the decision was made to export the pivoted df to .csv and continue the manipulations in Tableau - for both the allowed amount (that our sub-team is working on - Deigo, Teresa and me); and the other team who's working on the utilization change (Ari, Nicole and Jack).\n",
    "\n",
    "#### Reminder of questions our sub-team is answering: \n",
    "    - Goals 2: Which procedures had the largest change in Average payment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in 2015 pickle file (clean; doesn't have the irrelevant row)\n",
    "\n",
    "df_payments_2015 = pd.read_pickle('../data/pickled_files/payments_2015.pkl')\n",
    "print(df_payments_2015.shape)\n",
    "df_payments_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in 2016 pickle file (clean; doesn't have the irrelevant row)\n",
    "\n",
    "df_payments_2016 = pd.read_pickle('../data/pickled_files/payments_2016.pkl')\n",
    "print(df_payments_2016.shape)\n",
    "df_payments_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in 2017 pickle file (clean; doesn't have the irrelevant row)\n",
    "\n",
    "df_payments_2017 = pd.read_pickle('../data/pickled_files/payments_2017.pkl')\n",
    "print(df_payments_2017.shape)\n",
    "df_payments_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payments_2015.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new column with payment-type  (Doctor Only, Facility Only, or Doctor & Facility)\n",
    "(Code from Diego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 to add new column with payment type (Code from Deigo)\n",
    "\n",
    "conditions = [\n",
    "    (df_payments_2015.place_of_service == 'O'),\n",
    "    (df_payments_2015.entity_type_of_the_provider == 'I') & (df_payments_2015.place_of_service == 'F'),\n",
    "    (df_payments_2015.entity_type_of_the_provider == 'O') & (df_payments_2015.place_of_service == 'F')\n",
    "]\n",
    "\n",
    "choices = ['Doctor & Facility', 'Doctor Only', 'Facility Only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 to add new column with payment type (Code from Deigo)\n",
    "\n",
    "df_payments_2015['payment_type'] = np.select(conditions, choices, default = 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed that there are no \"unknown\" values.  THERE ARE UNKNOWNS. RESEARCHING FURTHER\n",
    "\n",
    "df_payments_2015['payment_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payments_2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmts_subset_2015 = df_payments_2015[['year',\n",
    "                                       'payment_type', \n",
    "                                       'hcpcs_code', \n",
    "                                       'average_medicare_allowed_amount'\n",
    "                                      ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmts_subset_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pmts_subset_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Takes a long time to run\n",
    "# This shows that there are a few outliers, but not in a usable way.\n",
    "\n",
    "print(df_pmts_subset_2015.plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarifying what is aggregated up into combined df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read in the combined df with all years, that was created in step03 notebook\n",
    "\n",
    "df_payments_combined = pd.read_pickle('../data/pickled_files/payments_combined.pkl')\n",
    "print(df_payments_combined.shape)\n",
    "df_payments_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payments_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pivot the df_payments_combined so that each year is column header (not a value under \"year\")\n",
    "# Code is from Diego\n",
    "\n",
    "pivot_index = ['national_provider_identifier',\n",
    "               'entity_type_of_the_provider', \n",
    "               'place_of_service',\n",
    "               'payment_type',\n",
    "               'provider_type',\n",
    "               'hcpcs_code',\n",
    "               'hcpcs_description',\n",
    "               'zip_code_of_the_provider', \n",
    "               'state_code_of_the_provider']\n",
    "\n",
    "pivot_cols = ['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pmt_pvt = df_payments_combined.pivot_table(index = pivot_index, \n",
    "                                              columns = pivot_cols, \n",
    "                                              values = 'average_medicare_allowed_amount', \n",
    "                                              aggfunc=np.mean)\n",
    "df_pmt_pvt = df_pmt_pvt.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape and head of the combined df. Looks correct. 20793075 rows, 12 columns.\n",
    "\n",
    "print(df_pmt_pvt.shape)\n",
    "df_pmt_pvt.head().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the type, to ensure it's a df.\n",
    "\n",
    "type(df_pmt_pvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmt_pvt.to_pickle('../data/pickled_files/payments_pivoted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmt_pvt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating df to get median based on pmt type and hcpcs code, by using \"groupby\" on pmt_type and hcpcs_code.\n",
    "# This didn't give the desired result - too many null values in year columns. \n",
    "# Back to the drawing board.\n",
    "\n",
    "df_pmt_pvt_group = df_pmt_pvt.groupby(['payment_type',\n",
    "                                          'hcpcs_code'])\n",
    "\n",
    "df_pmt_pvt_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next attempts: \n",
    "**FIRST: Create new column for type of payment in 2015**\n",
    "- Creating the new column for payment_type (Doctor & Facility, etc.) in the 2015 df, so we can use that to do groupby.\n",
    "- Successful in creating the column, but still unable to find what we needed from this.\n",
    "\n",
    "**SECOND: Try to create combined table in a different way:**\n",
    "- The next thing we did was add this column also to 2016 and 2017, to then MERGE them (instead of concatenating, then pivoting) to see if that would help with the calculations.  It still didn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding payment_type column to 2015\n",
    "\n",
    "conditions = [\n",
    "    (df_payments_2015.place_of_service == 'O'),\n",
    "    (df_payments_2015.entity_type_of_the_provider == 'I') & (df_payments_2015.place_of_service == 'F'),\n",
    "    (df_payments_2015.entity_type_of_the_provider == 'O') & (df_payments_2015.place_of_service == 'F')]\n",
    "\n",
    "choices = ['Doctor & Facility', 'Doctor Only', 'Facility Only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding payment_type column to 2015\n",
    "\n",
    "%%time\n",
    "df_payments_2015['payment_type'] = np.select(conditions, choices, default = 'unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding payment_type column to 2015\n",
    "\n",
    "df_payments_2015.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding payment_type column to 2016\n",
    "\n",
    "conditions = [\n",
    "    (df_payments_2016.place_of_service == 'O'),\n",
    "    (df_payments_2016.entity_type_of_the_provider == 'I') & (df_payments_2016.place_of_service == 'F'),\n",
    "    (df_payments_2016.entity_type_of_the_provider == 'O') & (df_payments_2016.place_of_service == 'F')]\n",
    "\n",
    "choices = ['Doctor & Facility', 'Doctor Only', 'Facility Only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding payment_type column to 2016\n",
    "\n",
    "%%time\n",
    "df_payments_2016['payment_type'] = np.select(conditions, choices, default = 'unknown')\n",
    "df_payments_2016.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding payment_type column to 2017\n",
    "\n",
    "conditions = [\n",
    "    (df_payments_2017.place_of_service == 'O'),\n",
    "    (df_payments_2017.entity_type_of_the_provider == 'I') & (df_payments_2017.place_of_service == 'F'),\n",
    "    (df_payments_2017.entity_type_of_the_provider == 'O') & (df_payments_2017.place_of_service == 'F')]\n",
    "\n",
    "choices = ['Doctor & Facility', 'Doctor Only', 'Facility Only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding payment_type column to 2017\n",
    "\n",
    "%%time\n",
    "df_payments_2017['payment_type'] = np.select(conditions, choices, default = 'unknown')\n",
    "df_payments_2017.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payments_2016.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns we don't need.\n",
    "# This code works, but takes a long time, so it's been commented out to prevent running when not needed.\n",
    "\n",
    "\n",
    "'''\n",
    "df_payments_2016_subset = df_payments_2016.drop(columns = ['last_name_organization_name_of_the_provider',\n",
    "                                                           'entity_type_of_the_provider',\n",
    "                                                           'city_of_the_provider',\n",
    "                                                           'zip_code_of_the_provider',\n",
    "                                                           'state_code_of_the_provider',\n",
    "                                                           'provider_type','hcpcs_description',\n",
    "                                                           'number_of_services',\n",
    "                                                           'number_of_medicare_beneficiaries',\n",
    "                                                           'number_of_distinct_medicare_beneficiary_per_day_services'])\n",
    "#                                                           'year'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge (outer) on 2015 and 2016. Our plan was to first merge these two, then merge 2017 with the new df.\n",
    "# But the merge didn't work well. Returned lots of duplicated columns with column_name_x and column_name_y.\n",
    "\n",
    "df_pmt_combined_merge = pd.merge(df_payments_2015,\n",
    "                                df_payments_2016,\n",
    "                                how='outer',\n",
    "                                on=['national_provider_identifier',\n",
    "                                    'hcpcs_code',\n",
    "                                    'payment_type'\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmt_combined_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Work on Friday 6/5/2020 - we intended to do the following. However, with live-coding session, we discovered that our original data (the combined df created a few days ago df_avg_pmt contained many duplicate rows. \n",
    "- We would need to go back and fix that before proceeding.  \n",
    "- The other sub-team had managed to solve the original issue in Tableau that had caused us to split off and try this method in the first place, so it wasn't a good use of time to try and fix this.  \n",
    "- DataFrames we would have created, given time and need:  \n",
    "**df_pmt_melt_amt** (average) 2-year change in ALLOWED AMOUNT, by AMOUNT  \n",
    "**df_pmt_melt_pct** (average) 2-year change in ALLOWED AMOUNT, by PERCENT  \n",
    "**df_util_melt_number** (sum) 2-year change in BEN/DAY SERVICES, by NUMBER  \n",
    "**df_util_melt_pct** (sum) 2-year change in BEN/DAY SERVICES, by PERCENT  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
